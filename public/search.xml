<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Min Heap Python Implementation]]></title>
    <url>%2F2019%2F10%2F23%2FMin-Heap-Python-Implementation%2F</url>
    <content type="text"><![CDATA[This is a python implementation of min-heap. Heap QueueWe use a complete binary tree to represent our heap. A complete binary tree can be stored in an array with list[parent] being the parent node of two children list[parent * 2] and list[parent * 2 + 1] &lt;heapq.py&gt; Implementationheapq is a python library provides an implementation of min heap, also known as the priority queue. This implementation uses zero-based indexing (the first element stored in heapq[0]). A full documentation can be referred here We will mainly use heappush and heappop to create our own wrapper. heapq.heappush(heap, item)Push the value item onto the heap, maintaining the heap invariant. heapq.heappop(heap)Pop and return the smallest item from the heap, maintaining the heap invariant. If the heap is empty, IndexError is raised. To access the smallest item without popping it, use heap[0]. import heapq class MinHeap: def __init__(self): self._heap = [] def __len__(self): return len(self._heap) def push(self, priority, item): &quot;&quot;&quot; Push an item with priority into the heap. Priority 0 is the highest. &quot;&quot;&quot; assert priority &gt;= 0 heapq.heappush(self._heap, (priority, item)) def pop(): &quot;&quot;&quot; Returns the item with lowest priority &quot;&quot;&quot; return heapq.heappop(self._heap)[1] # (priority, item)[1] == item Raw ImplementationWe maintain the heap in a index-one-based list (always keep heap[0] to be None). For all elements in the heap, heap[parent] will ideally have two children on heap[parent * 2] and heap[parent * 2 + 1]. Besides, in order to maintain the heap to be a complete binary tree, we should always insert to the end of the list and do some procedures to rearrange the new element to a proper place in our heap. For the insertion, we will always append to the list. Afterwards, bubble up the new element. Start from the last element (which is the newly inserted one), compare its value with its parent (index divided by two), if less than its parent, swap parent and child until its value is no longer less than its parent or hit the root of the heap. When calling pop, we pop heap[1], which is the minimum value out of the heap, replace heap[1] with the last element of the heap to maintain a complete binary tree. Afterwards, sink down the new root element to a proper palce in the heap. Precisely, compare the value of the root (previously the last element) and two of its child, if any of them is less than the root value, swap root and that child. Keeping doing this until none of its two child is less than itself or reach the leaf of the tree. Below is a complete implementation class MinHeap: def __init__(self): &quot;&quot;&quot; Init with index-one-based list &quot;&quot;&quot; self._heap = [None] def __len__(self): return len(self._heap) - 1 def _swap(self, t1, t2): self._heap[t1], self._heap[t2] = self._heap[t2], self._heap[t1] def _up_heap(self, index): &quot;&quot;&quot; Bubble up the inserted element to the proper place &quot;&quot;&quot; while index &gt; 1: parent = index // 2 if self._heap[parent] &gt; self._heap[index]: self._swap(parent, index) index = parent def _down_heap(self, index): &quot;&quot;&quot; Sink down the root element &quot;&quot;&quot; length = len(self._heap) while index * 2 &lt; length: child = index * 2 # compare two children, assign target child to be the smaller one if child + 1 &lt; length and self._heap[child+1] &lt; self._heap[child]: child += 1 if self._heap[child] &lt; self._heap[index]: self._swap(child, index) index = child def empty(): return len(self._heap) == 1 def push(self, value): &quot;&quot;&quot; Push a new element at the tail of the heap And bubble up &quot;&quot;&quot; if not value: raise TypeError(&#39;&quot;value&quot; cannot be of NoneType&#39;) self._heap.append(value) self._up_heap(len(self._heap) - 1) def pop(self): &quot;&quot;&quot; Pop the root element, which is the minimum value of the heap Assign the last element from the list to be the new root Then sink down &quot;&quot;&quot; if leb(self._heap) &lt;= 1: return None result = self._heap[1] last = self._heap.pop(-1) if len(self._heap) &gt; 1: self._heap[1] = last self._down_heap(1) return result def peak(self): if len(self._heap) &gt; 1: return self._heap[1] return None]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>Min Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LRU Cache Python Implementation]]></title>
    <url>%2F2019%2F10%2F22%2FLeast-Recently-Used-LRU-Cache-Implementation%2F</url>
    <content type="text"><![CDATA[This implementation is based on double-linked-list and dictionary. Runtime: O(1) Initinitialize the linked-list with a sentinel head and tailAnd an empty dictionary: dict = { } InsertSuppose we have a object to insert { keyA: value_a } Firstly we check out if out dictionary already contains a key named A (if so remove it from the linked list). Then append the new node to the tail of our linked-listAfterwards, add the new object to our dictionary key by keyA dict = { keyA: valueA } If we have already constructed the follwing linked-listThen to insert { keyA: valueA } once again (for example, the same object is used/modified again)We will firstly remove it from the current linked-list and append the same node at the tail of the list After the insertion, if the size of the list hit the maximum capacity, we need to pop the first element (which is not recently used), and assign sentinel head to the next element GetCheckout if our dictionary contains the key, if not, which means the element does not exist, return -1.If the key does exist, get the target node, then remove it from the linked-list and append at the tail Implementationclass Node: def __init__(self, k, v): self.key = k self.val = v self.prev = None self.next = None class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.dict = {} self.head = Node(0, 0) self.tail = Node(0, 0) self.head.next = self.tail self.tail.prev = self.head def get(self, key: int) -&gt; int: if key in self.dict: node = self.dict[key] self._remove(node) self._append(node) return node.val return -1 def put(self, key: int, value: int) -&gt; None: if key in self.dict: self._remove(self.dict[key]) node = Node(key, value) self._append(node) self.dict[key] = node if len(self.dict) &gt; self.capacity: n = self.head.next self._remove(n) del self.dict[n.key] def _remove(self, node: Node): p = node.prev n = node.next p.next = n n.prev = p def _append(self, node: Node): p = self.tail.prev p.next = node node.prev = p node.next = self.tail self.tail.prev = node]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Configure Kerberos Authentication in Golang]]></title>
    <url>%2F2019%2F10%2F10%2FConfigure-Kerberos-Authentication-in-Golang%2F</url>
    <content type="text"><![CDATA[This is a short guide about how to configure kerberos authentication on the client side (server side already been configured) using Golang Breif introduction of KerberosKerberos is a protocol developed by MIT scientists with the name introduced by the three-head dog from myth. It is well-spreaded as a general authorization technology and used across multiple platforms. The basic steps to get authentication are: A Client requests an authentication ticket(TGT) with credentials from the Key Distribution Center(KDC) KDC verifies the credentials and returns an encrypted TGT Client saves TGT and sends the encrypted TGT to the Ticket Granting Service(TGS) KDC verifies TGT and notifies TGS, then TGS returns a valid session token to the client Client uses the token to access a specific server ( If TGT expires, client will request for a new one by calling kinit ) Go ClientThere are multipe clients available for Golang, and you can refer to here to have a breif look. Usually we intent to choose libraries which are purely coded in the sme programming language to avoid importing unnecessary dependencies. Unfortunately, by the time I wrote this article, I haven’t found out a library which is purely written by Go, therefore I choose a cgo library supported by Confluent, this library refers to a C library librdkafka. EnvironmentFor the reason that I couldn’t refresh my keytab on MacOS (most likely because macos disable UDP connection by default and hence cannot recognize kdc in my realms), I switched to linux (for dockerfile, linux is also a good choice :P). The Go version is required to be at least 1.12 Firstly, since our client refering to librdkafka, we need to install librdkafka: wget -qO - https://packages.confluent.io/deb/5.3/archive.key | apt-key add - &amp;&amp; \ add-apt-repository &quot;deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main&quot; Then install: apt-get install -y librdkafka-dev I am going to use GSSAPI as SASL authentication and kerberos krb configuration hence need to import a few more tools: apt-get install -y libsasl2-modules-gssapi-mit libsasl2-dev apt-get install -yqq krb5-user libpam-krb5 libsasl2-modules-gssapi-mit and libsasl2-dev are specified to GSSAPI authentication, if you are using other mechanism (for example PLAIN or SCRAM-SHA-256), please refer to corresponding tools. And double check if you have ca-certificates installed (Not sure why, but without ca-certificates, krb configuration cannot be set) Create a Client in Goimport kafka &quot;github.com/confluentinc/confluent-kafka-go/kafka&quot; client, err := kafka.NewConsumer(&amp;kafka.ConfigMap{ // Avoid connecting to IPv6 brokers: // when using localhost brokers on OSX, since the OSX resolver // will return the IPv6 addresses first. // You typically don&#39;t need to specify this configuration property. // &quot;broker.address.family&quot;: &quot;v4&quot;, &quot;bootstrap.servers&quot;: &quot;[Server host:port]&quot;, &quot;group.id&quot;: &quot;[Group id]&quot;, &quot;security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;session.timeout.ms&quot;: 6000, &quot;sasl.mechanism&quot;: &quot;GSSAPI&quot;, &quot;auto.offset.reset&quot;: &quot;earliest&quot;, &quot;sasl.kerberos.service.name&quot;: &quot;[Service name]&quot;, &quot;sasl.kerberos.keytab&quot;: &quot;[Key tab location]&quot;, &quot;sasl.kerberos.principal&quot;: &quot;[Principal]&quot;, &quot;sasl.kerberos.kinit.cmd&quot;: &quot;kinit -R -t \&quot;%{sasl.kerberos.keytab}\&quot; -k %{sasl.kerberos.principal}&quot;, }) Then export JAAS configurations export KRB5_CONFIG=&quot;/usr/local/kafka/conf/krb5/krb5.conf&quot; export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/usr/local/kafka/conf/kafka/kafka_client_jaas.conf-Djava.security.krb5.conf=/usr/local/kafka/conf/krb5/krb5.conf -Dsun.security.krb5.debug=true&quot; For here, I use keytab to authorize which need to configured on kafka server side. Initially, we can use username and password to authorize. &quot;sasl.username&quot;: username, &quot;sasl.password&quot;: password, Other configuration fields can be referred to the offical documentaion DockerfileWell, if you don’t wanna spend time on the above configurations, here is the dockerfile :D # refer to a cgo library maintained by Confluent: https://github.com/confluentinc/confluent-kafka-go # which requires a C dependency librdkafka-dev: https://github.com/edenhill/librdkafka # The C dependency librdkafka-dev is curretly not available for other linux version except for ubuntu/debian. FROM ubuntu ENV http_proxy= ENV https_proxy= ENV DEBIAN_FRONTEND=noninteractive # Install the C lib for kafka RUN apt-get update &amp;&amp; \ apt-get install -y --no-install-recommends apt-utils wget gnupg software-properties-common &amp;&amp; \ apt-get install -y apt-transport-https ca-certificates git curl openssl libsasl2-modules-gssapi-mit libsasl2-dev &amp;&amp; \ apt-get install -yqq krb5-user libpam-krb5 &amp;&amp; \ # import source repository from confluent, check the latest version on https://docs.confluent.io/current/installation/installing_cp/deb-ubuntu.html#get-the-software wget -qO - https://packages.confluent.io/deb/5.3/archive.key | apt-key add - &amp;&amp; \ add-apt-repository &quot;deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main&quot; &amp;&amp; \ # import the librdkafka-dev from confluent source repository # confluent-kafka-go always requires the latest librdkafka-dev library # If go build fail below because of the mismatch of confluent-kafka-go and librdkafka-dev, # please check the latest source repositary on https://docs.confluent.io/current/installation/installing_cp/deb-ubuntu.html#get-the-software apt-get install -y librdkafka-dev &amp;&amp; \ # Install Go add-apt-repository ppa:longsleep/golang-backports &amp;&amp; \ apt-get install -y golang-1.12-go # build the library WORKDIR /src ADD . /src RUN GOPATH=/go GOOS=linux /usr/lib/go-1.12/bin/go build -o app &amp;&amp; \ mv /src/app /usr/local/bin ENV http_proxy &#39;&#39; ENV https_proxy &#39;&#39; EXPOSE 8000 ENTRYPOINT [&quot;/usr/local/bin/app&quot;]]]></content>
      <tags>
        <tag>Kafka</tag>
        <tag>Golang</tag>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Brew Error When Upgrade Mongodb to 4.2]]></title>
    <url>%2F2019%2F10%2F01%2FAn-Brew-Error-When-Upgrade-Mongodb-to-4-2%2F</url>
    <content type="text"><![CDATA[When I tried to upgrade my old mongodb to 4.2 through homebrew: brew tap mongodb/brew then: brew install mongodb-community@4.2 I encountered a strange error saying: Error: parent directory is world writable but not sticky That is because of the privilege issue of writing to /tmp,do the following will solve the problem: sudo chmod +t /private/tmp Remeber to link to the new version after the installation: brew link --overwrite mongodb-community]]></content>
      <tags>
        <tag>Brew</tag>
        <tag>Mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Suffix Tree]]></title>
    <url>%2F2019%2F07%2F22%2FSuffix-Tree%2F</url>
    <content type="text"><![CDATA[Suffix tree is a data structure aimming at solving string related problems in linear time String Match AlgorithmA common string match problem always contains: Text as an array of length n: T[n] Pattern as an array of length m which m &lt;=n: P[m] Valid Shift which is the offset of the first character of the pattern showing up in the target text We usually use two methods to solve string match problem: Naive String Matching Algorithm Knuth-Morris-Pratt Algorithm（KMP Algorithm) The algorithm always consists with two steps: Preprocessing and Matching, and the total runtime will be accordingly the sum of two procedures. Naive String Matching: O( (n-m)m )KMP: O( m + n ) Such algorithms are all doing preprocessings on the pattern to boost the searching procedure and the best runtime perfomance will be O(m) on preprocessing where m represents the length of the pattern. On contrary, is there a preprocessing which can be applied on text to speed up the whole process? This is the key reason why I am moving to suffix tree, which is a data structure doing preprocessing on text. Suffix TreeMy previous post used to introduce a prefix tree, for example:Individual nodes branch out from the same prefix. As you can see, there are some nodes which only have one child. Let’s try to compress them together:After the compression, we get a Compressed Prefix Tree. A compressed prefix tree, also called Compressed Tire is the fundamental of suffix tree. Besides, the key values stored in each nodes of a suffix tree is all the possible suffix. For example, for a single word (Text) banana\0, we have the following set of suffix: banana\0 anana\0 nana\0 ana\0 na\0 a\0 \0 Construct a prefix tree using the above key words:Afterwards, compress it:Here, by lising all the suffix and making a compressed prefix tree, we obtain a suffix tree. However, there is a faster way to construct a suffix tree which can be done in the linear time (By Esko Ukkonen in 1997). Let’s start from a simple example before exploring more complicated cases. “abc”Unlike prefix tree, the edge in suffix tree will no longer represent a single character(key); instead, it will represent a list of integers [from, to] which interprets the indexes in the original text. Therefore, each edge can represent abitrary length of characters and consume constant memory. Considering a simple string abc which has no duplicated character. Start from the first character a, construct an edge from root node to its leaf:Where [0, #] represents start from offset 0 and end up at offset #. Currently # equals to 1, which represents character a. After finishing the procudure of the first character, let’s move forward to the second one b:Then do the same thing on the last character c:We maintain a pointer pointing to the current end character of the text. Each time when we add a new edge for a new character, we will update the # point. Thus, the runtime for each procedure is O(1) and hence the overall runtime to construct this “abc” suffix tree will be O(n). “abcabxabcd”“abc” is a really simple example since there is no duplicated characters in the text. Now let’s consider a more complicated text abcabxabcd. Before we get started to construct, there are two more concepts that need to be introduced active point: a structure indicates active_node, active_value and offset remainder: remaining characters to be inserted active_point = (root, &#39;&#39;, 0) remainder = 0 The first character =&gt; aWe append a directly to the root node Before we append the node, set remainder = remainder + 1 (0 + 1), states that we are going to insert a new node Append ‘a’ to the root node After the insertion, a new node is appended to the tree, hence remainder = remainder - 1 (1 - 1) After the whole procudure:active_point = (root, &#39;&#39;, 0) remainder = 0 The second character =&gt; b Before we append the node, set remainder = remainder + 1 (0 + 1), states that we are going to insert a new node Expend every leave node; hence change ‘a’ to ‘ab’ Query the current node, which is the root node, since there is no prefix starts with ‘b’, we need to create a new node and insert ‘b’. (The prefix for the previous prefix will be [a, ab] now) After the insertion, a new node is appended to the tree, hence remainder = remainder - 1 (1 - 1) After the whole procudure:active_point = (root, &#39;&#39;, 0) remainder = 0 The third character =&gt; cSame as the procedure of inserting ‘b’ The fourth character =&gt; a againThis time we encounter the same character we have already inserted Before we append the node, set remainder = remainder + 1 (0 + 1) Expend every leave node; hence change ‘abc’ -&gt; ‘abca’, ‘bc’ -&gt; ‘bca’, ‘c’ -&gt; ‘ca’ Query the current node, which is the root node, find out the there already exist a prefix which starts from ‘a’; hence we modify the active_point:active_point = (root, &#39;a&#39;, 1) remainder = 1 active_node: the parent node is still root node hence remain unchangedactive_value: the current value will be set to ‘a’offset: the offset of the current value, which is 1 (states the first character in the prefix)remainder after the insertion, no new node is inserted in the tree, hence remainder keep the same (remainder = 1) The fifth character =&gt; b againStill duplicated with previous nodes remainder = remainder + 1 (1 + 1) Expend every leave node; hence change ‘abca’ -&gt; ‘abcab’, ‘bca’ -&gt; ‘bcab’, ‘ca’ -&gt; ‘cab’, active_point[1] = &#39;a&#39; -&gt; &#39;ab&#39; Start from active_node, query the active_value in its prefix and the prefix of its sub-nodes. Modify active_point to the following:active_point = (root, &#39;ab&#39;, 2) remainder = 2 active_node: the active node is still root node hence remain unchanged.active_value: the active value has been expended to ‘ab’.offset: ‘ab’ already exists in node ‘ab’ (which is the sub-node of root node), the offset of matched index is 2.remainder: after the insertion, no new node is inserted in the tree, hence remainder keep the same By this step, we still have the same amount of nodes inserted into the tree (3 nodes) The sixth character =&gt; x remainder = remainder + 1 (2 + 1). Expend every leave node; hence change ‘abcab’ -&gt; ‘abcabx’, ‘bcab’ -&gt; ‘bcabx’, ‘cab’ -&gt; ‘cabx’, active_point[1] = &#39;ab&#39; -&gt; &#39;abx&#39;. Start from active_node, query the active_value in its prefix and the prefix of its sub-nodes. Since there isn’t a prefix which perfectly matches the pattern ‘abx’; hence this is the point where to split: SplitWhen we find out the exact place where the prefix starts to mismatch the pattern, then we need to insert new nodes to our tree to append this new suffix character. For example, prefix ‘abc’ and pattern ‘abx’ share the same two characters ‘ab’ at the beginning and mismatch at the third character (offset = 3), then we can simply split at the third index to make a tree like ‘ab’ -&gt; [‘c’, ‘x’]. We will do the same thing here: We already know the offset (which is active_point[2]). Split node ‘abcabx’ to ‘ab’ -&gt; [‘cabx’, ‘x’]. Modify active_point and remainder:active_point = (root, &#39;bx&#39;, 1) remainder = 2 active_node: still root nodeactive_value: ‘abx’ has been inserted, then we need to insert ‘bx’, then ‘x’ and so on so faroffset: active_point[1] changes from ‘abx’ to ‘bx’, then the offset will also shift from 2 to 1remainder: a new node is inserted, hence remainder = remainder - 1 (3 - 1) Why do we change offset as well?This is actually a trick to reduce runtime. For each step of insertion, we will make an sub suffix tree by characters we already have (inserted). For example, if we have a string ‘abc’ and we have inserted the last character ‘c’ in the suffix tree, then all possible suffix of string (text) ‘abc’, which is ‘abc’, ‘bc’ and ‘c’ must exist in the suffix tree, either implicitly (have not splitted yet, like the fourth insertion above) or explicitly (has its own node). Hence, by inserting ‘ab +x’ and splitting the node, we know that ‘b +x’ must exist in the tree; therefore, we reduce the offset by 1 to state that the offset of the next split will start from the first character of the node ‘b…’. The remainder is still greater than 0; hence we will keep inserting ‘bx’. Do the same thing here, start from the parent_node, query the sub prefix ‘bx’. Found ‘bx’ in the prefix ‘bcabx’; hence split the node ‘bcabx’ to ‘b’ -&gt; [‘cabx’, ‘x’] Modify active_point and remainder:active_point = (root, &#39;x&#39;, 0) remainder = 1 active_node: still root nodeactive_value: ‘bx’ has been inserted, then we need to insert ‘x’offset: active_point[1] changes from ‘bx’ to ‘x’, then the offset will also shift from 1 to 0remainder: a new node is inserted, hence remainder = remainder - 1 (2 - 1) Since remainder is still greater than 0, we have to do another insertion to reduce it to zero. However, the character ‘x’ hasn’t been inserted before, then we append ‘x’ directly to the root node. Modify active_point and remainder:active_point = (root, &#39;&#39;, 0) remainder = 0 active_node: still root nodeactive_value: the last character ‘x’ has been inserted, reduce to emptyremainder: a new node is inserted, hence remainder = remainder - 1 (1- 1) Is there a relationship between two splitted node ab and b?When we separated ‘ab’ and ‘b’, we modified the previous ‘abcabx’ node first, then modified the second node ‘bcabx’. If we encounter another string starting with ‘abk’, then after ‘abk’ is inserted, we definately need to insert ‘bk’ as well. Repeating the same query will slow down the whole procedure; thus, we can add a link from ‘ab’ to ‘b’ to indicate that if an update is made in node ‘ab’, then another insertion will be done on the link target ‘b’ afterwards. The seventh character =&gt; aSame as the fourth insertion ‘a’ active_point = (root, &#39;a&#39;, 1) remainder = 1 The eighth character = bSame as the fifth insertion ‘b’ active_point = (root, &#39;ab&#39;, 2) remainder = 2 The ninth character =&gt; cWhen we try to insert ‘c’ remainder= remainder + 1 (3) Expand every leave node Query ‘abc’, find ‘abc’ under the parent node ‘ab’ Extra Procedure:active_point: The node ‘cabxabc’ constains the last character of the target prefix (‘abc’), whose parent node is ‘ab’; thus, set active_point[0] = &#39;ab&#39;.active_node: set active_point[1] = &#39;abc&#39;.offset: the offset of the character ‘c’ in its node string(‘cabxabc’) which is 1. Set active_point[2] = 1remainder: After the modification, no new node is inserted, hence remainder keeps unchanged. active_point = (&#39;ab&#39;, &#39;abc&#39;, 1) remainder = 3 The last character =&gt; dWe will use the link built-up in previous steps to simplify the updates. remainder = remainder + 1 (3 + 1) Expand every leave nodes Query the current parent node (‘ab’), find if there exist any prefix contains the target ‘abcd’? Since there isn’t any prefix match the pattern, then this will be a split point. Split active_point[0] = ‘ab’: start from the curent parent node (‘ab’), looking into its sub nodes.active_point[1] = ‘abcd’: ‘d’ is inserted into the active value .active_point[2] = 1: parent node ‘ab’ already contains ‘ab’, and its sub-node ‘cabxabcd’ starts from the third character ‘c’ at offset 0. Then the second character of ‘cabxabcd’, which is ‘a’ mismatches the following target ‘d’; hence this is where to be splitted, at offset 1 (of ‘cabxabcd’). Split ‘cabxabcd’ to c -&gt; [abxabcd, d] Since a new node has been inserted, then remainder = remainder - 1 (3). Remainder is still greater than 0, we still cannot stop here.active_point = (&#39;ab&#39;, &#39;abcd&#39;, 1) remainder = 3 active_point[0] (‘ab’) has a link pointing to ‘b’. We change the active_point according to the link target. active_point[0] = ‘b’active_point[1] = ‘bcd’active_point[2] remains unchange start from active_point[0], query its sub-nodes. ‘b’ is the parent node and its sub-node ‘cabxabcd’ contains ‘c’ at offset 0 and mismatch at offset 1; hence we split its sub-node ‘cabxabcd’ to ‘c’ -&gt; [abxabcd, d]. Since a new node is inserted, remainder = remainder - 1 (2) and active_point[1] = &#39;cd&#39; At this point, active_node reduce to empty and does not have any link, then we reset **active_point[0] = ‘root’ Insert ‘cd’, start from the current node (active_point[0]), which is the ‘root’, query its sub-nose for active_point[1] (‘cd’). Node ‘cabxabcd’ contains target character ‘c’ and mismatches at offset 1. Split ‘cabxabcd’ to c -&gt; [abxabcd, d]. remainder = remainder - 1 (1). Finally insert ‘d’ directly on root node and remainder = remainder - 1 which is 0]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>Suffix Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prefix Tree]]></title>
    <url>%2F2019%2F07%2F18%2FPrefix-Tree%2F</url>
    <content type="text"><![CDATA[A trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. Each node of the tree represents a string (prefix) and has multiple child nodes. A prefix tree is always used as search prompt when users browse through the search engine. The runtime of each query operation is independent with the size of the prefix tree; instead, it is decided by the length of the query target. SampleHere list an array of some random words: sin, sis, con, com, cin, cmd Then we can construct a prefix tree as following:During the query operation, there is no need to iterate the whole tree; instead, simply following each node and recursively going through its children will return the target or none if the target does not exist in the tree. Construct A Prefix TreeThere are multiple ways to form a prefix tree. The most popular method is to maintain three arrays in the instance to record end, path and next. Node class TrieNode: def __init__(self): self.nodes = {} self.count = 0 self.isword = False CRUD class Trie: def __init__(self): &quot;&quot;&quot; Initialize data structure. &quot;&quot;&quot; self.root = TrieNode() def insert(self, word: str): &quot;&quot;&quot; Inserts a word into the trie. :type word: str :rtype: void &quot;&quot;&quot; curr = self.root for char in word: if char not in curr.nodes: curr.nodes[char] = TrieNode() curr.nodes[char].count += 1 curr = curr.nodes[char] curr.isword = True def query(self, target: str): &quot;&quot;&quot; Returns if the word is in the trie. :type target: str :rtype: bool &quot;&quot;&quot; curr = self.root for char in target: if char not in curr.nodes: return False curr = curr.nodes[char] return curr.isword def startWith(self, prefix: str): &quot;&quot;&quot; Returns the number of words in the trie that start with the given prefix. :type prefix: str :rtype: int &quot;&quot;&quot; curr = self.root for char in prefix: if char not in curr.nodes: return 0 curr = curr.nodes[char] return curr.count def delete(self, target: str): &quot;&quot;&quot; Returns true if target exist and successfully delete from the trie. :type target: str :rtype: bool &quot;&quot;&quot; curr = self.root for char in prefix: if char not in curr.nodes: return False curr = curr.nodes[char] if curr.isword: curr.isword = False return True return False def listAllMatches(self, prefix: str): &quot;&quot;&quot; Returns all words started with prefix :param prefix: :return: List[str] &quot;&quot;&quot; result = [] def recursiveQuery(node: TrieNode, path: str): if not node.nodes: result.append(path) else: for key in node.nodes.keys(): recursiveQuery(node.nodes[key], path + key) curr = self.root for char in prefix: if char not in curr.nodes: return result curr = curr.nodes[char] recursiveQuery(&#39;&#39;) return result]]></content>
      <tags>
        <tag>Prefix Tree</tag>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Implement Zero Data Loss in Spark Streaming]]></title>
    <url>%2F2019%2F05%2F22%2FImplement-Zero-Data-Loss-in-Spark-Streaming%2F</url>
    <content type="text"><![CDATA[This is a log from my own experience in spark streaming during my work. Base on different environment and servers, the strategy may vary. For here, I am working with Kafka and Google Cloud PubSub. BackgroundI have such workflow that the spark streaming receives dataset from both Kafka and PubSub, after doing some clean up and modeling, push the data to the Cloud Datastore.It works fine when the data stream is small and reports me the expected values; however while the number of end users is growing large, especially when the data stream turns to be erratic and sometimes considerably large if end users interact frequently with our UI pages, the spark streaming will get a lot of uncertain runtime errors. Those errors are most likely caused by the finite number of workers in our cluster. Also when there are too much stream rushing over to the server, the limited memory in cache will cause failures. After we upgraded the cluster on cloud, the number of errors is significantly reduced.But the data processed during the errors was permanently lost and cannot be recovered. The data running on spark is buffered in memory (cache) and will be cleared meanwhile a failure occurs. This is not desired since some valuable KPIs may be lost as well. To prevent such data loss, I tried different strategies. CheckpointCheckpointing is a process supported by spark streaming after version which will save RDDs in log after being checkpointed. There are two level of checkpoints: reliable and local. Reliable checkpoint ensures that the data is stored permanentlly on HDFS, S3 or other distributed filesystems. Each job on cluster will create a directory for saving checkpointed logs, which will look pretty much like the directory below: ├── &quot;SomeUUID&quot; │ └── rdd-# │ ├── part-timestamp1 │ ├── .part-timestamp1.crc │ ├── part-timestamp2 │ └── .part-timestamp2.crc Since the data stream will be replicated on disk, the performance will slow down due to file I/O. Local checkpoint privileges performance over fault-tolerance which will persist RDDs on local storage in the executors. Read or write will be faster in this case; however if a driver fails, the data not yet executed may not be recoverable. As default, the data storeage level is set to MEMORY_AND_DISK which saves data in cache and disk (some in cache and some in disk). For here I changed to MEMORY_AND_DISK_SER_2 (more details can be referred to here). The different is that, unlike cache only, the checkpoints doesn’t save DAG with all the parents of RDDs; instead, they only save particular RDDs and remain for a longer time than cache. The time of persistance is strictly related to the executed computation and ends with the end of the spark application. To apply the checkpoint machanism, you just simply need to set the checkpoint directory when you are creating the StreamingContext def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } and before an action is operated on RDD: sRDD.checkpoint() sRDD.foreachPartition { partitionOfRecords =&gt; { ... } } and sRDD.isCheckpointed() will return true. For cleaning, the RDDs stored in cache will be cleaned with all other memory after the whole spark application is finished or terminated; the reliable RDDs stored on disk can be cleaned manually or setspark.cleaner.referenceTracking.cleanCheckpoints property to be true to enable automatic cleaning. This driver recovery mechanism is sufficient to ensure zero data loss if all data was reliably store in the system. However for my circumstance, the data is read from kafka and some of the data buffered in memory could be lost. If the driver process fails, all the executors running will be killed as well, along with any data in their memory. This pushes me to look for other mechanisms which are more advanced. Write Ahead LogsWrite Ahead Logs are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do. Spark streaming uses Receiver to read data from Kafka. They run as long-running tasks in the executors and store the revecived data in the memory of the executors. If you enable the checkpoint, the data will be checkpointed either in cache or disk in executors before porceed into the application drivers. Unlike checkpoint, applying WAL will instead backup the recevied data in an external fault-tolerant filesystem. And after the executor batches the received data and sends to the driver, WAL supports another log to store the block metadata into external filesystem before being executed.(diagram from https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)Spark streaming starts supporting WAL after version 1.2 and can be enabled by setting the config: val ssc = StreamingContext.createContext(...params) def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } Set the spark automatically clean checkpoints to release disk memory: val ssc = StreamingContext.createContext(...params) def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } This will not clean the latest checkpoint as it is still referred to by the application to recover from possible failures. If you expect to restart the application driver if it crashed due to some errors and exactly start from where it crashed last time instead of performing the whole operation once again, you can create StreamingContext from the previous checkpoint by using the method getOrCreate(): val ssc = StreamingContext.getOrCreate(CHECK_POINT_DIR, () =&gt; createContext(...params)) def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } However after I tried switching to getOrCreate(), I had the following exception: Exception in thread &quot;main&quot; org.apache.spark.SparkException: Failed to read checkpoint from directory checkpointDir at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:368) at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827) ... Caused by: java.io.IOException: java.lang.ClassCastException: cannot assign instance of com.some.project$$anonfun$1 to field org.apache.spark.streaming.dstream.MappedDStream.org$apache$spark$streaming$dstream$MappedDStream$$mapFunc of type scala.Function1 in instance of org.apache.spark.streaming.dstream.MappedDStream at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310) at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ... It is because this is not the first time I use checkpoints, and there already exsists some other checkpoints in the same root directory (etc, the simple checkpoint discussed previously). Since the application tried to read from those unrelated checkpoints and expected to cast them to generate new context, it would throw such ClassCastException. This can be easily solved by deleting original checkpoints in HDFS locals (manually). WAL is an advanced checkpoint mechanism and also simple to be applied. Compared with checkpoint, it saves data into an external filesystem so that even though if the executor is terminated and the data in memory is clean-uped, data still can be recovered from the external filesystem. However. it can only recover the data which is logged in the filesystem, if the drivers fail due to some error, the executor will be terminated as well, so as the WAL writer. Then the rest incomming data will not be logged into the filesystem and hence, is not recoverable. It can be tested by calling stop to the StreamContext: def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit Once it is called, the following console log interpretes that the WAL Writer is interrupted as well: ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver WARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll] WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted. And also the data will not be recoverable across applications or Spark upgrades and hence not very reliable Kafka Direct APIThis mechanism is only available when you data source is Kafka. Kafka supports a commit strategy which is able to help you manage offsets of each topics. Each offset points to a slot in a topic. When the data stored in this slot is consumed by any receiver, Kafka will be acknowledged by this consumption and moves the index to the next data slot.(diagram from https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/)When enable.auto.commit is set to be true, as soon as any receiver retrieves the data from the offset datastore in Kafka (here I use Kafka to store offsets), the receiver will automatically commit, which doesn’t ensure that the data is successfully executed in the spark streaming. Therefore, we have to disbale the auto-commit when we are creating DStream from Kafka. After the data is successfully processed, we manually commit the offset to the datastore by calling the Kafka direct API: stream.foreachRDD { rdd =&gt; //get current offset val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges //process data //store data in some datastore DataStoreDB.push(...param) //commit offset to kafka stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges) } } If an error occurs at any point during the execution, the offset won;t be committed to the kafka offset store and hence the same data will be resent by kafka, which ensures that the data will be executed only once.]]></content>
      <tags>
        <tag>Kafka</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用VS Code免翻墙听网易云音乐]]></title>
    <url>%2F2019%2F05%2F22%2F%E5%A6%82%E4%BD%95%E7%94%A8VS-Code%E5%85%8D%E7%BF%BB%E5%A2%99%E5%90%AC%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[在墙外打开网易云音乐发现全是灰色的？ 抱歉您所在区域无法播放？ 该资源暂无版权？ 需要vip？VS Code一行script全搞定～～ 打开VS Code 左侧 Extensions 搜索 Netease Music (VSC Netease Music) 或者点这里 点击 Install 后重启 VS Code 作者提供了基于 VS Code 自身插件工具 Webview 实现的通过替换 electron 动态链接库翻墙的插件, 有详细的中文文档, 这里 Unix Shell 用户（包括 MacOS）可以直接在 Terminal （任意dir）输入以下script: curl https://gist.githubusercontent.com/nondanee/f157bbbccecfe29e48d87273cd02e213/raw | python script 输出结果为: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 3060 100 3060 0 0 8496 0 --:--:-- --:--:-- --:--:-- 8476 vscode 1.34.0 x64 electron 3.1.8 download well replace done remove temp 替换完毕后打开 VS Code, 上方工具栏 Go -&gt; Go to file… 或者 Command(⌘) P, 输入： &gt;NeteaseMusic: Start 等待 editor 跳出 Please preserve this webview tab 后就可以使用所有网易云的功能了, 注意这个tab页面必须要保留（不能关闭） 使用时直接Command(⌘) P**后输入命令即可, 命令都是以 &gt;NeteaseMusic起头, 输入 &gt;ne 后VS Code会自动跳出并补齐可行指令, 甚至还可以登陆收藏查看评论]]></content>
      <tags>
        <tag>VS Code</tag>
        <tag>NeteaseMusic</tag>
        <tag>网易云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Slice in Golang]]></title>
    <url>%2F2019%2F05%2F12%2FType-slice-in-Golang%2F</url>
    <content type="text"><![CDATA[This article is a summary from Andrew Gerrand’s blog Golang has an unique type slice which is an abstraction built on top of Go’s array type. They are really similar but providing different means of working with sequences of typed data. So to understand slices we must first understand arrays. Arrays in GoAn array in Go has to specify its length and element type. The size of the array is fixed and its length is part of its type. For example [4]int and [5]int are distinct and have different types even though they all store integers. And contrary to C/C++, the initial value of an array will be filled with 0 if it is not initialized. var a [4]int a[0] = 1 i := a[0] j := a[1] //i == 1 //j == 0 Go’s arrays are values. An array variable denotes the entire array; it is not a pointer to the first array element (as would be the case in C/C++). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that’s a pointer to an array, not an array) An array literal can be specified like so: b := [2]string{&quot;aa&quot;, &quot;bb&quot;} Or, you can have the compiler counting the array elements for you: b := [...]string{&quot;aa&quot;, &quot;bb&quot;} In both cases, the type of b is [2]string. Slices in GoArrays are a bit inflexible, so you don’t see them too often in the code. Slices, though, are everywhere. Unlike an array type, a slice type has no specific length: b := []string{&quot;aa&quot;, &quot;bb&quot;} We can use build-in function make() to define a slice: func make([]T, len, cap) []T T represent the type of the elements. Function make accepts type, length and capacity(optional) as parameters. When it is called, make will allocate an array and returns a slice that refers to that array var s []byte s = make([]byte, 5, 5) //s == []byte{0, 0, 0, 0, 0} If cap is not specified, it will be init as the value of len. We can use the build-in functions len() and cap() to check the length and capacity of a slice: len(s) == 5 cap(s) == 5 The zero value of a slice is nil. The len and cap functions will both return 0 for a nil slice. A slice can also be formed by “slicing” an existing slice or array, for example, the expression b[1:4] creates a slice including elements 1 through 3 of b: b := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} // b[1:4] == []byte{&#39;b&#39;, &#39;c&#39;, &#39;d&#39;}, sharing the same storage as b The start and end indices of a slice expression are optional; they default to zero and the slice’s length respectively: // b[:2] == []byte{&#39;a&#39;, &#39;b&#39;} // b[2:] == []byte{&#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} // b[:] == b This is also the syntax to create a slice given an array: x := [3]string{&quot;Лайка&quot;, &quot;Белка&quot;, &quot;Стрелка&quot;} s := x[:] // a slice referencing the storage of x Slicing does not copy the slice’s data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements of a re-slice modifies the elements of the original slice: d := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;} e := d[2:] // e == []byte{&#39;c&#39;, &#39;d&#39;} // now change the re-slice will also change the original slice e[1] = &#39;m&#39; // e == []byte{&#39;c&#39;, &#39;m&#39;} // d == []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;m&#39;} A slice cannot be grown beyond its capacity. Attempting to do so will cause a runtime panic, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array. Double the capacity of a sliceTo increase the capacity of a slice, we must create a new, larger slice and copy the contents of the original slice into it. The belowing example shows how to create a new slice t whihc doubles the capacity of s: t := make([]byte, len(s), (cap(s) * 2)) for i:= range s { t[i] = s[i] } s = t //reassign s to t The loop can be replaced by the build-in function copy(), which copies the data from source and returns the number of elements copied: func copy(dst, src []T) int The function copy supports copying between slices of different lengths (it will copy only up to the smaller number of elements) and the case that two slices refer to the same array. Using copy, the above double size code snippet can be rewritten as: t := make([]byte, len(s), (cap(s) * 2)) copy(t, s) s = t A common operation is to append new data to the tail of a slice: func AppendByte(slice []byte, data ...type) []byte { m := len(slice) n := m + len(data) if n &gt; cap(slice) { //if the original capacity is not big enough newSlice := make([]byte, (n + 1) * 2) copy(newSlice, slice) slice = newSlice } slice = slice[0:n] //shrink the capacity to the length of data copy(slice[m:n], data) return slice } This customized AppendByte function is really useful because we can fully control the size of a slice. However most programs do need such complete control. Go provides a build-in function append() which appends slice x to the end of slice s, expanding s if needed: func append(s []T, x ...T) []T Using … to append one slice to the end of another: a := []string{&quot;aa&quot;, &quot;bb&quot;} b := []string{&quot;cc&quot;, &quot;dd&quot;} a = append(a, b...) //same as append(a, b[0], b[1], b[2]) Another example of append: func Filter(s []int, fn func(int) bool) []int { var p []int // p == nil for _, v := range s { if fn(v) { p = append(p, v) } } return p }]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>Array</tag>
        <tag>Slice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to debug NodeJS on VS Code]]></title>
    <url>%2F2019%2F05%2F08%2FHow-to-debug-NodeJS-on-VS-Code%2F</url>
    <content type="text"><![CDATA[Here are the steps to start debug mode in VS Code: On the left side bar, click “debug” icon to switch to debug viewlet On the top left, click the gear icon Then launch.json will be opened in the editor Replace the content of the file to be: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;name&quot;: &quot;Launch app.js&quot;, &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;, &quot;stopOnEntry&quot;: true, &quot;args&quot;: [ &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot; ] } ] } Replace the command line arguments to whatever you need Start the debugger or press F5 You are all good to go! If your program reads from stdin, please add a “console” attribute to the launch config: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;name&quot;: &quot;Launch app.js&quot;, &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;, &quot;stopOnEntry&quot;: true, &quot;args&quot;: [ &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot; ], &quot;console&quot;: &quot;integratedTerminal&quot; } ] } If you are running the program in the terminal, you can change the content alternatively to be: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node&quot;, &quot;request&quot;: &quot;attach&quot;, &quot;name&quot;: &quot;Attach to app.js&quot;, &quot;port&quot;: &quot;5858&quot; } ] } The port is the debug port and it has nothing to do with your program (no matter it is a service or not). Then in the terminal, run the command: node --debug-brk app.js arg1 arg2 arg3... The --debug-brk lets your program wait for the debugger to attach to. So there is no problem that it terminates before the debugger could attach. Running such command, you may encounter a warning below: (node:31245) [DEP0062] DeprecationWarning: `node --inspect --debug-brk` is deprecated. Please use `node --inspect-brk` instead. As discussed in microsoft github offical repository, currently there is no way to prevent this happening. The reason why using --inspect --debug-brk is explained here: This combination of args is the only way to enter debug mode across all node versions. At some point I’ll switch to inspect-brk if we don’t want to support node 6.x anymore, or will do version detection for it and do something for runtimeExecutable scenarios. The problem is that we do not really know what version of node a user is using, so we cannot adapt the flags we use to the node version in order to minimize the resulting deprecation warnings.]]></content>
      <tags>
        <tag>VS Code</tag>
        <tag>NodeJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Check Open TCP/IP Ports in Mac OS X]]></title>
    <url>%2F2019%2F05%2F06%2FHow-to-Check-Open-TCP-IP-Ports-in-Mac-OS-X%2F</url>
    <content type="text"><![CDATA[The core of Mac OS is Darwin and we can use most of the CLI tools in Mac OS just like how it feels like in Linux. If we want to check out the current ports in usage, the command netstat is useful: netstat -ap tcp | grep -i &quot;listen&quot; That will print out something like this in the console: Achive Internet connections(including servers) Proto Recv-Q Send-Q Local Address Foreign Address (state) tcp4 0 0 localhost.25035 *.* LISTEN That works but the problem is that it doesn’t show up the names of the procedures which occupy the ports. Sometimes we want to know precisely which program is exposing the port. Then found out that there is another command lsof: sudo lsof -nP -iTCP:PortNumber -sTCP:LISTEN which prints out all the processes running in a given port with specific names: COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME syslogd 350 root 5w VREG 222,5 0 440818 /var/adm/messages syslogd 350 root 6w VREG 222,5 339098 6248 /var/log/syslog cron 353 root cwd VDIR 222,5 512 254550 /var -- atjobs -n : No dns (no host name)-P : List port number instead of its name-i : Lists IP sockets To view the port associated with a daemon: lsof -i -n -P | grep python If we just want to see the name: sudo lsof -i :PortNumber | grep LISTEN Get all running PID in a specific port: sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID And then we can kill all the processes: sudo kill -9 $(sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID) list all commands: lsof -h]]></content>
      <tags>
        <tag>CLI</tag>
        <tag>Mac OS</tag>
        <tag>port</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Memory Leaks in Serveral Commonly Used Programming Languages]]></title>
    <url>%2F2019%2F05%2F05%2FMemory-Leak-in-Serveral-Commonly-Used-Programming-Languages%2F</url>
    <content type="text"><![CDATA[Usually when we talk about memory leaks we are actually talking about the memory leaks in heap memory. When an object is initialized, it will be dynamically allocated to a piece of memory in the heap and ready to be manipulated. After we perform some operations and the whole procedure is finished, the object stored in heap should also be erased; however in the case of memory leak, that piece of memory is not released but still held in the heap, marked as occupied but no reference refers to it. Wiki’s Def: Memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that: memory which is no longer needed is not released an object is stored in memory but cannot be accessed by the running code We usually encounter this issue in programming languages that don’t have GC, for example C++ and C. For such languages, we have to manage the memory by ourselves which, if not done properly, will expose the risks of memory leaks. This is really common in C++Let’s take a look in C++. There are literally hundreds of ways that can cause memory leaks and most of them won’t be detected during compilation and even in runtime. Only a few leaks will not have any impact on the system; however if we are running a huge application and those leaks accumulate, that will significantly reduce the real runtime performance of the whole system. We all know that when we allocate an object, we have to release the memory if this object is not used anymore. The way we release the memory is simply call the buid-in function free() or delete[]. However in C++ the procedure can exit anywhere. An exception can be thrown in the half way so that the code doesn’t ever reach the line to release memory: int sample(int n) { void *ptr = malloc(16); if (n) return -1; //memory leak here free(ptr); return 0; } or: class Sample { public: init() { int *n = new int; throw any_exception(); } ~init() { delete n; } private: int *n; }; Sample *n = new Sample; //memory leak here The solution to the above examples is also really simple: check control flows and do remember to call the destructor before anywhere the procedure may exit. Well if you want to do it in a fancy way, you can use smart pointer alternatively: class Sample { public: init() { n = std::make_shared&lt;int&gt;(new int) } ~init() {} private: std::shared_ptr&lt;int&gt; n; }; Smart pointer helps you manage this object and if it is not referred anymore, release its memory. free( )/delete is not enoughNow your program has such a concrete control flow that free( ) or delete is called before all the possible drop out. That is great but still not enough. free( ) and delete can only release the memory where the pointer is currently pointing to but not the pointer itself! The pointer will still point to the original memory address but the content has been already removed. In this circumstance, the value of the pointer does not equal to NULL, instead some random values that cannot be predicted. int main() { char *p = (char*) malloc(sizeof(char) * 100); strcpy(p, &quot;hello&quot;); free(p); if (p != NULL) //doesn&#39;t prevent issue strcpy(p, &quot;world&quot;); // error } This pointer p is called dangling pointer or wild pointer and will only be erased after the whole procedure is finished or terminated. The wild pointer is really risky because of its random behavior. Imagine there is something in your room that sometimes can be observed sometimes cannot, randomly breaks your stuff but never leaves footprint. In programming it is called wild pointer, and in real life it is called cat. To prevent it, we should always set the pointer to be NULL when it is not used/the memory is released. Note: when you define a pointer without setting up its initial value, that pointer will also be a wild pointer and has a value of some random number (which doesn’t equal to NULL). Hence it is necessary to set the value of a pointer to be NULL if it cannot be asigned a value at the beginning. For some simple pointers, they can be reasigned to NULL to prevent wild pointer, however for a pointer referring to a hierarchical object, simply setting to NULL cannot resolve the potential issues. For example, you are using a vector in C++ : vector &lt;string&gt; v int main() { for (int i=0; i&lt;1000000; i++) v.push_back(&quot;test&quot;); cout &lt;&lt; v.capacity() &lt;&lt; endl; //memory usage: 54M v.clear(); cout &lt;&lt; v.capacity() &lt;&lt; endl; //memory usage: still 54M } Even though we have cleared the vector and all its elements were indeed released, the capacity of the vector is still unchanged. clear() removed all its element but cannot shrink the size of the container. The same thing happens to other containers such as deque. To handle this, before C++ 11, we can swap the pointers: int main() { ... v.clear(); vector&lt;string&gt;(v).swap(v); //new a vector with the same content and swap cout &lt;&lt; v.capacity() &lt;&lt; endl; //memory usage: 0 } after C++ 11, it provides function shrink_to_fit() to remove the extra allocated memory. GC doesn’t avoid memory leaksIt’s not suprising that GC can prevent most cases of memory leaks because it is runnig in an individual thread, checking the memory regularly and removing the unreferred objects. It is so powerful that porgrammers rarely pay attention to memory management and be aware of the memory leaks. Java is such language which has powerful and unruly GC that can be hardly controlled (call System.gc() doesn’t certainly invoke GC). It helps to manage the memory in jvm, but it cannot avoid memory leaks. There are mainly two cases that can lead to memory leaks in Java. One is the object which has a longer lifecycle keeps a reference to another object which has a shorter lifecycle: public class Sample { Object object; public void anymethod(){ object = new Object(); ... } ... } If object is only used inside anymethod( ), then after stack pops anymethod( ), the lifecycle of object should also be ended. But for here, because class Sample is still proceeding and keeps the reference of object, object cannot be collected by GC and hence leaks the memory. The solution will be either init object inside anymethod( ) (as a local varible) or set object to be null after anymethod is finished. Another case is the use of HashSet. HashSet is the implementation of hash-table and it stores elements according to their different hash values. In order to push and withdraw the same object in the HashSet, we need to override the method HashCode() so that the same object has the same hash vaule and being stored in the same place in HashSet. However, if we push something into the HashSet and then change some properties of this object (those properties are most likely to be used to calculate the hashcode), the hashcode of this object may vary and when we refer this object back to our HashSet to do some operations, for example delete this object from the HashSet, this object might not be found in the set and hence cannot be deleted: HashSet&lt;Obejct&gt; set = new HashSet&lt;Object&gt;(); Object something = new Object(); set.add(something); something.doSomethingChanges(); set.contains(something); //this may return false set.remove(something); //&#39;something&#39; cannot be removed if the previous line returns false Python]]></content>
      <tags>
        <tag>NodeJS</tag>
        <tag>C++</tag>
        <tag>Java</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prefix Notation]]></title>
    <url>%2F2019%2F04%2F30%2FPrefix-Notation%2F</url>
    <content type="text"><![CDATA[( 20 + 5 ) ( 16 / 4 ) Such expressions which denote procedures, are called combinations. The left and the right elements are called operands, and the element in the middle to indicate the operation is called operator. This is the most common style we have seen by now; however there is another way to construct a procedure known as prefix notation: ( + 20 5 ) ( / 16 4 ) Instead of injecting the operator between operands, which is a more human readable style, the prefix notation requires the operator always to be at the left most. conditions: ( define ( abs x ) ( cond (( &gt; x 0 ) x ) (( = x 0 ) 0 ) (( &lt; x 0 ) ( - x )))) The general form can be expressed as: ( cond (&lt;\P1&gt; &lt;\E1&gt;) (&lt;\P2&gt; &lt;\E2&gt;) … (&lt;\Pn&gt; &lt;\En&gt;)) If none of them is evaluated to be true, then the value of the cond will be undefined. It can also be simplified by using else: ( define ( abs x ) ( cond (( &lt; x 0 ) ( - x )) ( else x ))) If there is only two predicates (the expression to be interpreted as either true of false), then it can use a special form if: ( define ( abs x ) ( if ( &lt; x 0 ) ( - x ) x )) The general form of an if expression is: ( if &lt;\predicate&gt; &lt;\consequent&gt; &lt;\alternative&gt; ) The logic operators: ( and &lt;\E1&gt; … &lt;\En&gt; )( or &lt;\E1&gt; … &lt;\En&gt; )( not ) Then use the logic operators to define a predicate to evaluate if a number id larger or equal to the other one: ( define ( &gt;= x y ) ( or ( &gt; x y ) ( = x y )) That is all the syntax, there is no loop in a functional programming language! RecursionConsidering the factorial function: n! = n ⋅ (n-1) ⋅ (n-2) ⋅ … ⋅2⋅1 Which can be computed as: n! = n ⋅ (n-1)! If we end it up with 1!, then simply output 1. Then the factorial function can be implemented in linear recursion: ( define ( factorial n ) ( if ( = n 1 ) 1 ( * n ( factorial ( - n 1 ))))) Linear recursion defines that the computation chains of operations is proportional to n and hence grows linearly. There is also another pattern of recursion, known as Tree Recursion. The best example will be the Fibonacci series, in which each element is the sum of the previous two: ( define ( fib n ) ( cond ( = n 0 ) 0 ) ( = n 1 ) 1 ) ( else ( + ( fib ( - n 1 ) ) ( fib ( - n 2 ) ))))) You may find out that this procedure is not really efficient because to compute fib( - n 1), fib( - n 2) has to be computed one more time which causes duplicated work.Therefore, instead of Tree Recursion, let’s try to convert it to be Linear Recursion. Reasign the sum of a and b to a, and the previous a to b: ( define ( fib n ) ( iterate 1 0 n )) ( define ( iterate a b count ) ( if ( = count 0 ) b ( iterate ( + a b ) a ( - count 1 )))) LambdaInstead of defining some trivial procedures so that we can pass them as arguments of the other procedures, functional programming provides Lambda Expression: ( lambda ( &lt;\formal-param&gt; ) &lt;\body&gt; ) For instance, ( define ( Add a b ) ( + a b )) can be written as: ( define add ( lambda ( a b ) ( + a b ))) And operators can also be represented by Lambda Expression: (( lambda ( a b ) ( + ( * a a ) ( * b b ))) 2 3 ) Another use of Lambda Expression is creating local variables. An expression can be binded with a specific name by using keyword let. The above example then can be interpreted as: ( define ( sumsqr x y ) ( let ( a ( * x x )) ( b ( * y y )) ( + a b ))) Note: The scope of a variable specified by a let is only applied to the body of the let. For example, if the evalue of x is 2, then the expression: ( let (( x 3 ) ( y ( + x 2 ))) ( * x y )) The value of y will be 4 as being outside of the let body, and the output will be 3 * 4 = 12. It seems like let is really similar to define; however, in the most cases, we much prefer using let and only apply define to internal procedures.]]></content>
      <tags>
        <tag>Lisp</tag>
        <tag>Scheme</tag>
        <tag>Prefix Notation</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Brief Introduce to Redux Saga]]></title>
    <url>%2F2019%2F04%2F28%2FA-Brief-Introduce-to-Redux-Saga%2F</url>
    <content type="text"><![CDATA[If you are quite experienced with redux, which is a predictable state container for JavaScript applications (Note: even thouth React and Redux is a popular combination to build fast and powerful apps, Redux is not necessarily combined with React), you are definitely feeling comfortable with its powerful store which manages all the global states and provides much cleaner logic flows to change them. If you are new to redux, here is the guide to dive before we start our topic. In a complex javascript application, asynchronous function is always one of the most annoying part where encounters tons of bugs. If not handle them properly, the app usually ends up with call back hell. Haven’t heard of CallBack Hell?Well, in javascript, the only way you can suspend a computation and have the rest operations doing later is to put the rest operations into a callback function. This callback function usually returns a Promise (And has a type of Promise&lt;any&gt;). In order to easily mark those async functions, after ES6 javascript provides extra modifiers async and await, which actually wraps up the original utilities of promise and makes it more readable to programmers. Hummm, sounds like things are going better… NO!! It doesn’t resolve anything! The core problem leads to a callback hell is the hierarchical async calls, for example you have some simple synchronous functions which are in a chain to accomplish some logics: a = getSomething(); b = getMore(a); c = getMoreAndMore(b); ... It looks fine for now, but what if they all turn out to be async? Then you have to follow the callback style to make them operate one right after another is done: getSomthing(function(a) { getMore(a, function(b) { getMoreAndMore(b, function(c) { //keep going... }); }); }); Or you prefer ES6: async function getSomething(a) { await b = ToDo(a); return await getMore((b) =&gt; { return await ToDo(b); }).then((c) =&gt; { return await ToDo(c); }).then(...); } Looks really confused? This will getting even uglier if we are using callbacks in loops. Redux ThunksBack to our redux app, we usually want to update some states after an async call to inform the UI that the data is ready to be fetched. That is always achieved by dispatching an action from the component to the reducer: async const callAPI = () =&gt; { ... return response; }; ... async const updateUI = (...params) =&gt; { const res = await callAPI(); if (res.status === 200) dispatch({type: &quot;UPDATE&quot;, isSuccess: true}); }; ... render({ ... this.props.isSuccess? showData() : showError() }); This isn’t bad, but we are always looking for something better. An advanced way to rewrite it is using redux middleware. Middleware is somewhere you can put the code after the framework receives a request and before it generates a response. For example, we want to add a logger in the redux store so that when the store catches an action, before it returns the new state, the logger can log the previous state and the new generated state. This is what can be added as a middleware: function logger(store) { return function wrapDispatch (next) { return function dispatchAndLog (action) { console.log(&quot;dispatching.. &quot;, action); let result = next(action); console.log(&quot;new state&quot;, store.getState()); return result; } } } There are more advanced ways to add a logger. If you are interested, please refer to the offical documentation. With our middleware, the previous example can be written in a cleaner way: const callAPI = () =&gt; { return((dispatch) =&gt; { dispatch(startCallingApiAction); actualCallApi().then(data =&gt; { dispatch(successAction(data)); }).fail(err =&gt; { dispatch(failedAction(err)); }); }); }; The successful response data is wrapped in the payload of the action, sent to the reducer. Once the store updates the data, it will be mapped as a prop back to the component and request for a rerender. This middleware is also called thunk. By applying thunk to decouple the presentation layer, we can get rid of most of the side effects in components, instead, managing and orchestrating side effects in thunks. This is great, so why are we even considering saga? Well, one of the advantages of middleware is that it can be chained. Every middleware mounted in redux store starts an individual thread (or something really looks like a thread in NodeJS). When a middleware captures an action and handles its side effect, it can dispatch a new action to another middleware to do nested logics. This behavior of middleware indicates that thunks can be chained as well, for example thunkA forwards its return payload to thunkB and thunkB forwards its return payload to… Wait! That sounds quite familiar!! Is that the case of callback hell?? Unfortunately, a good thing plus another good feature doesn’t always end up with something better. It could be some shit as well (笑) In this case, true, this is exactly the callback hell. Redux SagaTo handle the possible endless callback functions and also to make it more easily to test in a component which has complicated logics, we need to change our previous thoughts. Just like shifting from Process Oriented Programming to Object Oriented Programming, instead of telling the application how to handle the side effects, suppose it already knows how to call a function and how to dispatch an action, all we need to do is to give instructions about what to do next and we don’t care about how those instructions will be executed (Saga handles the executions). Then the thunks example can be changed as following: export function* apiSideEffect(action) { try{ const data = yield call(actualCallApi); yield put({ type:&quot;SUCCESS&quot;, payload: data }); } catch(err) { yield put({ type:&quot;FAILED&quot;, payload: err }); } } export function* apiSaga() { yield takeEvery(&quot;CLICK_TO_CALL_API&quot;, apiSideEffect); } There are serval fucntions already being integrated in Saga: Call: the method call will return only a plain object describing the operation so redux-saga can take care of the invocation and returns the result to the generator. The first parameter is the generator function ready to be called and the rest params are all the arguments in the generator. Put: Instead of dispatching an action inside the generator (Don’t ever ever do that), put Returns an object with instructions for the middleware to dispatch the action. Select: Returns value from the selector function, similar with getState(). Note: It is not recommended to use this function because it returns the value corresponding to the contents of the store state tree, which is most likely a plain Javascript object and is mutable (Redux wants you to handle state immutably, which means return a new state instead of changing the old one). Take: It creates a command object that tells the middleware to wait for a specific action. The resulting behavior of the call Effect is the same as when the middleware suspends the generator until a promise resolves. In the take case, it’ll suspend the generator until a matching action is dispatched By working with Saga, we make the side effects to be declarative rather than imperative. Declarative: describing what the program must accomplish, rather than describe how to accomplish it Imperative: consists of commands for the computer to perform, focuses on describing how a program operates In the case of take, the control is inverted. Instead of the actions being pushed to the handler tasks, the Saga is pulling the action by itself. An additional generator, known as watcher which contains take has to be created to watch a specific action and being triggered once the following action is dispatched in the application. There are two ways to create a watcher, one is using the buid-in functions (Saga Helper): function* watchFetchData() { yield takeEvery(&quot;FETCH_REQUEST&quot;, callFetchDataApi); } takeEvery allows multiple request to be proceeding at the same time. Or if you just want the latest request to be fired (the older one will be overrided during each time the watcher is triggered function* watchFetchData() { yield takeLatest(&quot;FETCH_REQUEST&quot;, callFetchDataApi); } However by using take, it is possible to fully control an action observation process to build complex control flow: function* watchFetchData() { while(true) { const action = yield take(&quot;FETCH_REQUEST&quot;); console.log(action); yield call(callFetchDataApi, action.payload); } } All right, now you have been exposed to everything you need to know before start trying redux saga on your own. Here is a short overall example that may also help:Store: const sagaMiddleware = createSagaMiddleware(); const store = createStore(rootReducer, appluMiddleware(sagaMiddleware)); sagaMiddleware.run(watchFetch); Sagas: function* watchFetch(): Generator&lt;*, *, *&gt; { yield takeEvery(&quot;FETCH_ACTION&quot;, callFetchAPI); } function* callFetchAPI(): Generator&lt;*, *, *&gt; { try { yield put({ type: &quot;FETCHING&quot;, payload: ... }); const data = yield call(actualCallApi); yield put({ type: &quot;FETCH_SUCCESS&quot;, payload: data }); } catch(err) { yield put({ type: &quot;FETCH_FAILED&quot;, payload: err }); } } Reducer: const reducer = (state = initState, action) =&gt; { switch(action) { case &quot;FETCHING&quot;: return { loading: true, ...state }; case &quot;FETCH_SUCCESS&quot;: return { loading: false, success: true, data: action.payload, ...state }; case &quot;FETCH_FAILED&quot;: return { loading: false, success: false, error: true, ...state }; default: return { ...state }; } }; Component: class myComponent extends React.Component { const mapStateToProps = ... const mapDispatchToProps = ... render() { return ( &lt;button onClick = { () =&gt; this.props.dispatch({type: &quot;FETCH_ACTION&quot;}) }/&gt; { this.props.loading? &lt;p&gt;Loading..&lt;/p&gt; : this.props.error? &lt;p&gt;Error!&lt;/p&gt; : &lt;p&gt;{this.props.data}&lt;/p&gt; } ); } } export default connect(mapStateToProps, mapDispatchToProps)(myComponent); For more advanced concepts, there is a well-organized Saga offical documentation you can refer to if you want to dive deeper. How to test Saga?A function that returns a simple object is easier to test than a function that directly makes an asynchronous call. For redux saga, each time you yield a function call will return a plain javascript object which makes the workflow much easier to test. You don’t need to use the real API, fake it, or mock it, instead just iterating over the generator function, asserting for equality on the values yielded. describe(&quot;fetch work flow&quot;, () =&gt; { const generator = cloneableGenerator(callFetchAPI)({ type: &quot;FETCH_ACTION&quot; }); expect(generator.next().value).toEqual(put({ type: &quot;FETCHING&quot;, payload: ... })); test(&quot;fetch success&quot;, () =&gt; { const clone = generator.clone(); expect(clone.next().value).toEqual(put({ type: &quot;FETCH_SUCCESS&quot; })); expect(generator.next().done).toEqual(true); }); }); In the above example, we use clone() to test different control flows and next() to iterate to the next function ready be yielded. The mock return value can also be injected as an argument of next(): expect(clone.next(false).value).toEqual( put(fetchFailedAction()) ); Saga vs ObservablesRedux saga is not the only solution to our apps which may have complex control flows, they are other helpful tools providing different trade-offs which can also resolve the async problems. Here are some good code snippets of saga vs observables that can open your mind :D References:https://redux-saga.js.org/https://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-ithttps://redux.js.org/advanced/middlewarehttps://pub.dartlang.org/packages/redux_thunkhttps://codeburst.io/how-i-test-redux-saga-fcc425cda018https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1https://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/https://redux.js.org/introduction/getting-startedhttps://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71]]></content>
      <tags>
        <tag>Redux</tag>
        <tag>Saga</tag>
        <tag>React</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
